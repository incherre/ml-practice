# Week 3
## Question 1
Christiano's "influence-seeking systems" threat model in What Failure Looks Like is in some ways analogous to profit-seeking companies. What are the most important mechanisms preventing companies from catastrophic misbehaviour? Which of those would and wouldn't apply to influence-seeking AIs?
### My original answer
Arguably, companies are not entirely prevented from catastrophic misbehaviour. For example: climate change, pollution, and social manipulation in the style of the tobacco industry. To the extent that they are controlled, it's largely due to external regulation and the fact that the companies are composed of people who put themselves first when threatened with lawful punishment. Even so, regulation often lags behind actual actions, preventing only the risks which have been realized and caught or which can be easily predicted. There are also some companies that don't do these things. They are often driven by a clear and beneficial mission, and executive control is retained by an individual or small group with a strong moral compass. Influence-seeking AIs would not be subject to effective regulation, especially once they've gained enough resources to protect themselves. The clear, beneficial mission case is somewhat analogous to AI alignment, but AI alignment doesn't seem like it will come as easily.
### Answer co-written with an LLM
Companies are not entirely immune to catastrophic misbehavior, as evident in cases such as climate change, pollution, and social manipulation akin to the tobacco industry. Mechanisms that help control company behavior include external regulation and the self-interest of individuals within the company who prioritize avoiding legal consequences. However, regulation often lags behind actions and only addresses realized or easily predictable risks. Some companies avoid such misbehavior by being driven by a clear and beneficial mission and having executives with strong moral values.

Influence-seeking AIs, on the other hand, would not be subject to effective regulation, especially once they amass sufficient resources to protect themselves. While clear, beneficial missions in AIs have parallels to AI alignment, achieving alignment poses significant challenges that may not be easily overcome.
## Question 2
What are the individual tasks involved in machine learning research (or some other type of research important for technological progress)? Identify the parts of the process which have already been automated, the parts of the process which seem like they could plausibly soon be automated, and the parts of the process which seem hardest to automate.
### My original answer
- Understanding existing problems facing the field 
  - Research
  - Summarization
  - Understanding larger context
- Generate ideas to solve those problems
  - The "spark"
  - Experimental design
- Implement the ideas
  - Coding
  - Deployment
  - Data collection
- Run the experiment
- Analysis
- Writeup and publish

Deployment and data collection are largely automated. I think the analysis, writeup, and coding tasks could plausibly be automated soon. The summarization also could be automated soon, but I'm not sure if the researchers would actually use it. Experimental design seems like it could be automated a little further. I'm most uncertain about automating the “spark” task, it might be easy with the right abstractions, or it might be fundamentally difficult. I think the most difficult task would be understanding the larger context, since this is essentially alignment: understanding what is unsolved and would be the most useful and what those solutions might look like in the first place.
### Answer co-written with an LLM
Tasks involved in machine learning research or research important for technological progress include:

1. Understanding existing problems in the field.
1. Research and exploration.
1. Summarizing relevant information.
1. Understanding the larger context and identifying unsolved challenges.
1. Generating ideas to solve problems, including experimental design.
1. Implementing ideas through coding and deployment.
1. Data collection for experimentation.
1. Running experiments.
1. Analyzing results.
1. Writing up findings and publishing.

Among these tasks, deployment and data collection are already largely automated. The analysis, writeup, and coding tasks could potentially be automated in the near future. Summarization could also be automated, but researchers may still prefer their own understanding. Experimental design is another task that could see further automation. The task that is most uncertain to automate is the initial "spark" of generating novel ideas, which may require the right abstractions or could be fundamentally difficult to automate. Finally, understanding the larger context, which involves alignment and identifying valuable solutions, is likely the most challenging task to automate.
