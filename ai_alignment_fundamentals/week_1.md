# Week 1
## Question 1
A crucial feature of AGI is that it will possess cognitive skills which are useful across a range of tasks, rather than just the tasks it was trained to perform. Which cognitive skills did humans evolve because they were useful in our ancestral environments, which have remained useful in our modern environment? Which have become less useful?
### My original answer
Humans evolved the ability to detect potential threats and resources in our environment primarily via vision and hearing. We evolved memory, the ability to throw things (predict and control parabolic arcs), the ability to communicate and teach, the ability to create tools.

Out of these, I think the most useful today are perceiving our environment, memory, creating tools, and communication. The ability to throw things is not that useful for many people, but still quite useful for some people.
### Answer co-written with an LLM
Humans evolved several cognitive skills that were beneficial in ancestral environments and continue to be useful in our modern environment. These include:
1. Perception: Humans developed the ability to detect potential threats and resources using vision and hearing.
1. Memory: The capacity to remember past experiences and information aids in decision-making and learning.
1. Tool creation: Humans evolved the ability to create and use tools, enhancing our problem-solving capabilities.
1. Communication: The skill to communicate and teach others facilitates cooperation and knowledge transfer.

These listed skills remain highly useful in our modern environment. One skill less salient in the modern world is the ability to throw things which, while less universally applicable, can still have practical value in specific contexts.
## Question 2
What are the most plausible ways for the hypothesis "we will eventually build AGIs which have transformative impacts on the world" to be false? How likely are they?
### My original answer
- Extinction: we die from some natural or unnatural disaster before we can finish.
- Internal coordination: we collectively decide that it’s a bad idea and then stick to that decision.
- External coordination: aliens come and tell us it’s a bad idea and prevent us from finishing.
- Difficulty: it turns out to be extremely difficult, or impossible.

I think Extinction is somewhat more likely than the other three. Internal Coordination far more likely than the final two. And the final two roughly equally as likely.
### Answer co-written with an LLM
There are several plausible ways in which the hypothesis of building transformative AGIs could be false:
1. Extinction: Human civilization could face natural or unnatural disasters leading to our extinction before AGIs are developed. This possibility may be somewhat more likely than the others.
1. Internal Coordination: There could be a collective decision by humans that building AGIs is a bad idea, and we adhere to this decision. This scenario is more likely than the following two.
1. External Coordination: The arrival of extraterrestrial beings or advanced civilizations warning against AGI development could prevent us from completing it. This possibility carries a relatively lower likelihood.
1. Difficulty: It might turn out that building AGIs is extremely difficult or even impossible. The likelihood of this scenario is comparable to the possibility of external coordination.
